{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot MCP Platform - Exercise Notebook\n",
    "\n",
    "This notebook exercises the RAG Chatbot MCP Platform by:\n",
    "1. Loading sample documents\n",
    "2. Chunking documents\n",
    "3. Creating embeddings\n",
    "4. Storing in vector database\n",
    "5. Retrieving relevant chunks\n",
    "6. Generating responses using Ollama\n",
    "\n",
    "## Prerequisites\n",
    "- **ChromaDB running** (Docker recommended): `docker-compose up -d chromadb`\n",
    "- **Ollama running locally**: `ollama serve`\n",
    "- **Smallest working model**: `ollama pull llama3.2:1b` or `ollama pull tinyllama`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ChromaDB availability...\n",
      "‚ö†Ô∏è ChromaDB has compatibility issues with Python 3.14+\n",
      "   Note: This is a known issue with pydantic v1\n",
      "   Recommendation: Use Python 3.11 or 3.12 for full ChromaDB support\n",
      "   Current: Will use mock mode (works for testing)\n",
      "   Will use mock mode (limited functionality)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConfigError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\servers\\mcp-vectorstore\\src\\server.py:35\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HttpClient\n\u001b[32m     36\u001b[39m     CHROMADB_HTTP_AVAILABLE = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\chromadb\\__init__.py:3\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client \u001b[38;5;28;01mas\u001b[39;00m ClientCreator\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AdminClient \u001b[38;5;28;01mas\u001b[39;00m AdminClientCreator,\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\chromadb\\api\\__init__.py:51\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollection_configuration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     CreateCollectionConfiguration,\n\u001b[32m     49\u001b[39m     UpdateCollectionConfiguration,\n\u001b[32m     50\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_DATABASE, DEFAULT_TENANT\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     53\u001b[39m     CollectionMetadata,\n\u001b[32m     54\u001b[39m     Documents,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     DefaultEmbeddingFunction,\n\u001b[32m     73\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\chromadb\\config.py:120\u001b[39m\n\u001b[32m    117\u001b[39m     ID = \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSettings\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseSettings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Generic config\u001b[39;49;00m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ==============\u001b[39;49;00m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pydantic\\v1\\main.py:221\u001b[39m, in \u001b[36mModelMetaclass.__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m validate_field_name(bases, var_name)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m inferred = \u001b[43mModelField\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUndefined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m fields:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pydantic\\v1\\fields.py:504\u001b[39m, in \u001b[36mModelField.infer\u001b[39m\u001b[34m(cls, name, value, annotation, class_validators, config)\u001b[39m\n\u001b[32m    502\u001b[39m annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pydantic\\v1\\fields.py:434\u001b[39m, in \u001b[36mModelField.__init__\u001b[39m\u001b[34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28mself\u001b[39m.model_config.prepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pydantic\\v1\\fields.py:544\u001b[39m, in \u001b[36mModelField.prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03mPrepare the field but inspecting self.default, self.type_ etc.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03mNote: this method is **not** idempotent (because _type_analysis is not idempotent),\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03me.g. calling it it multiple times may modify the field and configure it incorrectly.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_default_and_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m ForwardRef \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m DeferredType:\n\u001b[32m    546\u001b[39m     \u001b[38;5;66;03m# self.type_ is currently a ForwardRef and there's nothing we can do now,\u001b[39;00m\n\u001b[32m    547\u001b[39m     \u001b[38;5;66;03m# user will need to call model.update_forward_refs()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pydantic\\v1\\fields.py:576\u001b[39m, in \u001b[36mModelField._set_default_and_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type_ \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors_.ConfigError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munable to infer type for attribute \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.required \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mConfigError\u001b[39m: unable to infer type for attribute \"chroma_server_nofile\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\servers\\mcp-vectorstore\\src\\server.py:38\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mlogger\u001b[49m.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHttpClient import failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m     CHROMADB_HTTP_AVAILABLE = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     62\u001b[39m chunker_module = load_module_from_path(\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchunker_server\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     64\u001b[39m     project_root / \u001b[33m\"\u001b[39m\u001b[33mservers\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mmcp-chunker\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mserver.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m embeddings_module = load_module_from_path(\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33membeddings_server\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m     project_root / \u001b[33m\"\u001b[39m\u001b[33mservers\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mmcp-embeddings\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mserver.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m vectorstore_module = \u001b[43mload_module_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvectorstore_server\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmcp-vectorstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mserver.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     73\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m retriever_module = load_module_from_path(\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretriever_server\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m     project_root / \u001b[33m\"\u001b[39m\u001b[33mservers\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mmcp-retriever\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mserver.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Extract managers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_module_from_path\u001b[39m\u001b[34m(module_name, file_path)\u001b[39m\n\u001b[32m     13\u001b[39m spec = importlib.util.spec_from_file_location(module_name, file_path)\n\u001b[32m     14\u001b[39m module = importlib.util.module_from_spec(spec)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:759\u001b[39m, in \u001b[36m_LoaderBasics.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcannot load module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m when \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    758\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mget_code() returns None\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_with_frames_removed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:491\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_with_frames_removed\u001b[39m(f, *args, **kwds):\n\u001b[32m    484\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"remove_importlib_frames in import.c will always remove sequences\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m    of importlib frames that end with a call to this function\u001b[39;00m\n\u001b[32m    486\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m \u001b[33;03m    module code)\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\servers\\mcp-vectorstore\\src\\server.py:53\u001b[39m\n\u001b[32m     51\u001b[39m                 logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing ChromaDB HTTP client only (Docker mode)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[43mlogger\u001b[49m.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChromaDB import error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m     CHROMADB_AVAILABLE = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     55\u001b[39m     CHROMADB_HTTP_AVAILABLE = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import importlib.util\n",
    "\n",
    "# Add parent directories to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Helper function to load modules with hyphens in path\n",
    "def load_module_from_path(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Check ChromaDB availability\n",
    "print(\"Checking ChromaDB availability...\")\n",
    "CHROMADB_AVAILABLE = False\n",
    "CHROMADB_MODE = \"Mock (not available)\"\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")  # Suppress pydantic v1 warnings on Python 3.14+\n",
    "    \n",
    "    try:\n",
    "        # Try HttpClient first (for Docker connection)\n",
    "        try:\n",
    "            from chromadb import HttpClient\n",
    "            try:\n",
    "                # Try to connect to ChromaDB server (Docker)\n",
    "                test_client = HttpClient(host=\"localhost\", port=8000)\n",
    "                test_client.list_collections()  # Test connection\n",
    "                CHROMADB_AVAILABLE = True\n",
    "                CHROMADB_MODE = \"Docker (HTTP)\"\n",
    "                print(\"‚úÖ ChromaDB server detected at localhost:8000\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è ChromaDB server at localhost:8000 not responding: {e}\")\n",
    "                print(\"   Make sure Docker container is running: docker-compose up -d chromadb\")\n",
    "                print(\"   Will use mock mode (limited functionality)\")\n",
    "        except (ImportError, Exception) as e:\n",
    "            error_msg = str(e)\n",
    "            if \"pydantic\" in error_msg.lower() or \"ConfigError\" in str(type(e).__name__):\n",
    "                print(\"‚ö†Ô∏è ChromaDB has compatibility issues with Python 3.14+\")\n",
    "                print(\"   Note: This is a known issue with pydantic v1\")\n",
    "                print(\"   Recommendation: Use Python 3.11 or 3.12 for full ChromaDB support\")\n",
    "                print(\"   Current: Will use mock mode (works for testing)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è ChromaDB import failed: {error_msg}\")\n",
    "                print(\"   Install with: pip install chromadb-client\")\n",
    "            print(\"   Will use mock mode (limited functionality)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking ChromaDB: {e}\")\n",
    "        print(\"   Will use mock mode (limited functionality)\")\n",
    "\n",
    "# Load MCP server managers\n",
    "datasources_module = load_module_from_path(\n",
    "    \"datasources_server\",\n",
    "    project_root / \"servers\" / \"mcp-datasources\" / \"src\" / \"server.py\"\n",
    ")\n",
    "chunker_module = load_module_from_path(\n",
    "    \"chunker_server\",\n",
    "    project_root / \"servers\" / \"mcp-chunker\" / \"src\" / \"server.py\"\n",
    ")\n",
    "embeddings_module = load_module_from_path(\n",
    "    \"embeddings_server\",\n",
    "    project_root / \"servers\" / \"mcp-embeddings\" / \"src\" / \"server.py\"\n",
    ")\n",
    "vectorstore_module = load_module_from_path(\n",
    "    \"vectorstore_server\",\n",
    "    project_root / \"servers\" / \"mcp-vectorstore\" / \"src\" / \"server.py\"\n",
    ")\n",
    "retriever_module = load_module_from_path(\n",
    "    \"retriever_server\",\n",
    "    project_root / \"servers\" / \"mcp-retriever\" / \"src\" / \"server.py\"\n",
    ")\n",
    "\n",
    "# Extract managers\n",
    "DataSourcesManager = datasources_module.DataSourcesManager\n",
    "ChunkerManager = chunker_module.ChunkerManager\n",
    "EmbeddingManager = embeddings_module.EmbeddingManager\n",
    "VectorStoreManager = vectorstore_module.VectorStoreManager\n",
    "RetrieverManager = retriever_module.RetrieverManager\n",
    "\n",
    "# Ollama integration\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "    print(\"‚úÖ Ollama available\")\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Ollama not installed. Install with: pip install ollama\")\n",
    "\n",
    "print(f\"\\n‚úÖ All imports successful!\")\n",
    "print(f\"üìä ChromaDB Mode: {CHROMADB_MODE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Managers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:vectorstore_server:Backend chromadb not available, using mock mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing managers...\n",
      "‚úÖ All managers initialized!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "COLLECTION_NAME = \"rag_documents\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Local model\n",
    "OLLAMA_MODEL = \"llama3.2:1b\"  # Smallest working model\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K = 3\n",
    "\n",
    "# Initialize managers\n",
    "print(\"Initializing managers...\")\n",
    "datasource_manager = DataSourcesManager()\n",
    "chunker_manager = ChunkerManager(\n",
    "    default_chunk_size=CHUNK_SIZE,\n",
    "    default_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "embedding_manager = EmbeddingManager(default_model=EMBEDDING_MODEL)\n",
    "vectorstore_manager = VectorStoreManager(\n",
    "    backend=\"chromadb\",\n",
    "    persist_dir=\"./chroma_db\",\n",
    "    chroma_host=\"localhost\",\n",
    "    chroma_port=8000\n",
    ")\n",
    "retriever_manager = RetrieverManager()\n",
    "\n",
    "print(\"‚úÖ All managers initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: C:\\Users\\learn\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\data\\sample-data\n",
      "‚úÖ Loaded 2 documents!\n",
      "\n",
      "Document 1:\n",
      "  Path: C:\\Users\\learn\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\data\\sample-data\\faq.md\n",
      "  Type: markdown\n",
      "  Size: 3721 characters\n",
      "  Preview: # Frequently Asked Questions\n",
      "\n",
      "## General Questions\n",
      "\n",
      "### What is the RAG Chatbot MCP Platform?\n",
      "The RA...\n",
      "\n",
      "Document 2:\n",
      "  Path: C:\\Users\\learn\\Documents\\projects\\learn-rag-chatbot-mcp\\rag-chatbot-mcp\\data\\sample-data\\rag-overview.md\n",
      "  Type: markdown\n",
      "  Size: 3313 characters\n",
      "  Preview: # RAG Chatbot Knowledge Base - Sample Document\n",
      "\n",
      "## Introduction to RAG (Retrieval-Augmented Generati...\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents\n",
    "sample_data_path = Path().absolute().parent.parent / \"data\" / \"sample-data\"\n",
    "print(f\"Loading documents from: {sample_data_path}\")\n",
    "\n",
    "load_result = datasource_manager.load_files(\n",
    "    str(sample_data_path),\n",
    "    pattern=\"*.md\",\n",
    "    recursive=False\n",
    ")\n",
    "\n",
    "if load_result[\"status\"] == \"success\":\n",
    "    print(f\"‚úÖ Loaded {load_result['count']} documents!\")\n",
    "    loaded_documents = load_result[\"documents\"]\n",
    "    \n",
    "    # Display document info\n",
    "    for i, doc in enumerate(loaded_documents):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(f\"  Path: {doc.get('path', 'N/A')}\")\n",
    "        print(f\"  Type: {doc.get('type', 'N/A')}\")\n",
    "        print(f\"  Size: {doc.get('size', 0)} characters\")\n",
    "        print(f\"  Preview: {doc.get('content', '')[:100]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå Error loading documents: {load_result.get('message', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Chunk Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents...\n",
      "  Document 1: 9 chunks\n",
      "  Document 2: 10 chunks\n",
      "\n",
      "‚úÖ Created 19 total chunks from 2 documents\n",
      "Sample chunk (first 200 chars): # Frequently Asked Questions\n",
      "\n",
      "## General Questions\n",
      "\n",
      "### What is the RAG Chatbot MCP Platform?\n",
      "The RAG Chatbot MCP Platform is a modular system for building retrieval-augmented generation chatbots usin...\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "print(\"Chunking documents...\")\n",
    "for doc_idx, doc in enumerate(loaded_documents):\n",
    "    content = doc.get(\"content\", \"\")\n",
    "    metadata = doc.get(\"metadata\", {})\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunk_result = chunker_manager.chunk_text(\n",
    "        text=content,\n",
    "        strategy=\"recursive\",\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    chunks_data = chunk_result.get(\"chunks\", [])\n",
    "    print(f\"  Document {doc_idx+1}: {len(chunks_data)} chunks\")\n",
    "    \n",
    "    for chunk_idx, chunk_data in enumerate(chunks_data):\n",
    "        # Extract text from chunk data structure\n",
    "        chunk_text = chunk_data.get(\"text\", \"\") if isinstance(chunk_data, dict) else chunk_data\n",
    "        all_chunks.append(chunk_text)\n",
    "        all_metadatas.append({\n",
    "            **metadata,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"document_index\": doc_idx\n",
    "        })\n",
    "        all_ids.append(f\"doc_{doc_idx}_chunk_{chunk_idx}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(all_chunks)} total chunks from {len(loaded_documents)} documents\")\n",
    "if all_chunks:\n",
    "    print(f\"Sample chunk (first 200 chars): {all_chunks[0][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings using model: all-MiniLM-L6-v2\n",
      "This may take a moment...\n",
      "‚úÖ Created 19 embeddings\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for all chunks\n",
    "print(f\"Creating embeddings using model: {EMBEDDING_MODEL}\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "embed_result = embedding_manager.embed_batch(\n",
    "    texts=all_chunks,\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "if embed_result[\"status\"] == \"success\":\n",
    "    embeddings = embed_result[\"embeddings\"]\n",
    "    print(f\"‚úÖ Created {len(embeddings)} embeddings\")\n",
    "    print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error creating embeddings: {embed_result.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store in Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: rag_documents\n",
      "Collection creation: success\n",
      "Adding documents to vector store...\n",
      "‚úÖ Indexed 19 chunks!\n",
      "\n",
      "Collection Stats:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"collection\": \"rag_documents\",\n",
      "  \"count\": 19,\n",
      "  \"mode\": \"mock\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create collection\n",
    "print(f\"Creating collection: {COLLECTION_NAME}\")\n",
    "coll_result = vectorstore_manager.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_dimension=len(embeddings[0]) if embeddings else 384\n",
    ")\n",
    "print(f\"Collection creation: {coll_result['status']}\")\n",
    "\n",
    "# Add documents to collection\n",
    "print(\"Adding documents to vector store...\")\n",
    "add_result = vectorstore_manager.add_documents(\n",
    "    collection=COLLECTION_NAME,\n",
    "    documents=all_chunks,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=all_metadatas,\n",
    "    ids=all_ids\n",
    ")\n",
    "\n",
    "if add_result[\"status\"] == \"success\":\n",
    "    print(f\"‚úÖ Indexed {add_result['added']} chunks!\")\n",
    "    \n",
    "    # Get collection stats\n",
    "    stats = vectorstore_manager.get_collection_stats(collection=COLLECTION_NAME)\n",
    "    print(f\"\\nCollection Stats:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "else:\n",
    "    print(f\"‚ùå Error indexing: {add_result.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query the RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG and how does it work?\n",
      "\n",
      "1. Creating query embedding...\n",
      "‚úÖ Query embedding created (dimension: 384)\n",
      "\n",
      "2. Retrieving top 3 relevant chunks...\n",
      "‚úÖ Retrieved 3 documents\n",
      "\n",
      "Retrieved Documents:\n",
      "\n",
      "[1] Distance: 0.1000\n",
      "    Text: # Frequently Asked Questions\n",
      "\n",
      "## General Questions\n",
      "\n",
      "### What is the RAG Chatbot MCP Platform?\n",
      "The RAG Chatbot MCP Platform is a modular system for building retrieval-augmented generation chatbots usin...\n",
      "\n",
      "[2] Distance: 0.2000\n",
      "    Text: ### Who should use this platform?\n",
      "- **AI/ML Engineers** building production RAG systems\n",
      "- **Developers** exploring MCP and agentic AI\n",
      "- **Researchers** experimenting with retrieval techniques\n",
      "- **Stud...\n",
      "\n",
      "[3] Distance: 0.3000\n",
      "    Text: ### How do I install the platform?\n",
      "1. Clone the repository\n",
      "2. Run `./tests/manual_testing/Solution-Testing/install_dependencies.sh`\n",
      "3. Configure your environment variables\n",
      "4. Verify with `./tests/manu...\n",
      "\n",
      "3. Generating response using Ollama (llama3.2:1b)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESPONSE:\n",
      "============================================================\n",
      "RAG stands for Retrieval-Augmented Generation, which refers to a type of artificial intelligence (AI) system that combines the power of retrieval techniques with generation capabilities. In essence, RAG chatbots are designed to learn from a vast amount of text data by first retrieving relevant information and then generating new text based on that knowledge.\n",
      "\n",
      "The RAG Chatbot MCP Platform is specifically designed for building such systems using the Model Context Protocol (MCP). It provides seven specialized servers that handle different aspects of the RAG pipeline, making it a modular and scalable solution for building retrieval-augmented generation chatbots.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define a query\n",
    "query = \"What is RAG and how does it work?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Step 1: Create query embedding\n",
    "print(\"1. Creating query embedding...\")\n",
    "query_embed_result = embedding_manager.embed_text(\n",
    "    text=query,\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "if query_embed_result[\"status\"] != \"success\":\n",
    "    print(f\"‚ùå Error creating query embedding\")\n",
    "else:\n",
    "    query_embedding = query_embed_result[\"embedding\"]\n",
    "    print(f\"‚úÖ Query embedding created (dimension: {len(query_embedding)})\")\n",
    "    \n",
    "    # Step 2: Retrieve relevant chunks\n",
    "    print(f\"\\n2. Retrieving top {TOP_K} relevant chunks...\")\n",
    "    search_result = vectorstore_manager.search_similar(\n",
    "        collection=COLLECTION_NAME,\n",
    "        query_embedding=query_embedding,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    if search_result[\"status\"] != \"success\":\n",
    "        print(f\"‚ùå Error retrieving documents\")\n",
    "    else:\n",
    "        retrieved_docs = search_result[\"results\"]\n",
    "        print(f\"‚úÖ Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "        \n",
    "        # Display retrieved documents\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\n[{i}] Distance: {doc.get('distance', 0):.4f}\")\n",
    "            print(f\"    Text: {doc.get('document', '')[:200]}...\")\n",
    "        \n",
    "        # Step 3: Format context for generation\n",
    "        context_docs = []\n",
    "        for doc in retrieved_docs:\n",
    "            context_docs.append({\n",
    "                \"text\": doc.get(\"document\", \"\"),\n",
    "                \"metadata\": doc.get(\"metadata\", {})\n",
    "            })\n",
    "        \n",
    "        # Step 4: Generate response with Ollama\n",
    "        print(f\"\\n3. Generating response using Ollama ({OLLAMA_MODEL})...\")\n",
    "        \n",
    "        if not OLLAMA_AVAILABLE:\n",
    "            print(\"‚ùå Ollama not available. Install with: pip install ollama\")\n",
    "        else:\n",
    "            # Format prompt\n",
    "            context_str = \"\\n\\n\".join([\n",
    "                f\"[{i+1}] {doc['text']}\" \n",
    "                for i, doc in enumerate(context_docs)\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            try:\n",
    "                # Call Ollama\n",
    "                ollama_response = ollama.generate(\n",
    "                    model=OLLAMA_MODEL,\n",
    "                    prompt=prompt\n",
    "                )\n",
    "                \n",
    "                response_text = ollama_response[\"response\"]\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"RESPONSE:\")\n",
    "                print(\"=\"*60)\n",
    "                print(response_text)\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Ollama error: {e}\")\n",
    "                print(\"Make sure Ollama is running and the model is available.\")\n",
    "                print(f\"Try: ollama pull {OLLAMA_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Try Different Queries\n",
    "\n",
    "You can modify the query in the cell above and re-run it, or create new query cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper function defined. Use query_rag('your question') to query the system.\n"
     ]
    }
   ],
   "source": [
    "# Helper function for querying\n",
    "def query_rag(question, top_k=TOP_K, model=OLLAMA_MODEL):\n",
    "    \"\"\"Query the RAG system with a question.\"\"\"\n",
    "    print(f\"Query: {question}\\n\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embed_result = embedding_manager.embed_text(\n",
    "        text=question,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    \n",
    "    if query_embed_result[\"status\"] != \"success\":\n",
    "        return f\"Error creating query embedding\"\n",
    "    \n",
    "    query_embedding = query_embed_result[\"embedding\"]\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    search_result = vectorstore_manager.search_similar(\n",
    "        collection=COLLECTION_NAME,\n",
    "        query_embedding=query_embedding,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    if search_result[\"status\"] != \"success\":\n",
    "        return f\"Error retrieving documents\"\n",
    "    \n",
    "    retrieved_docs = search_result[\"results\"]\n",
    "    \n",
    "    # Format context\n",
    "    context_str = \"\\n\\n\".join([\n",
    "        f\"[{i+1}] {doc.get('document', '')}\" \n",
    "        for i, doc in enumerate(retrieved_docs)\n",
    "    ])\n",
    "    \n",
    "    # Generate with Ollama\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        return \"Ollama not available\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        ollama_response = ollama.generate(model=model, prompt=prompt)\n",
    "        return ollama_response[\"response\"], retrieved_docs\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", []\n",
    "\n",
    "print(\"‚úÖ Helper function defined. Use query_rag('your question') to query the system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Try another query\n",
    "response, sources = query_rag(\"What are the benefits of RAG?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìé Retrieved {len(sources)} source documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Loading documents from the sample data directory\n",
    "2. ‚úÖ Chunking documents using recursive strategy\n",
    "3. ‚úÖ Creating embeddings using sentence-transformers\n",
    "4. ‚úÖ Storing documents in ChromaDB vector store\n",
    "5. ‚úÖ Retrieving relevant chunks using semantic search\n",
    "6. ‚úÖ Generating responses using Ollama (smallest working model)\n",
    "\n",
    "The RAG pipeline is now fully functional! You can:\n",
    "- Load your own documents\n",
    "- Experiment with different chunk sizes and strategies\n",
    "- Try different embedding models\n",
    "- Query the system with various questions\n",
    "- Use different Ollama models for generation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
